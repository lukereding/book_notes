# Notes on _Upstream_

Dan Heath

The most interesting points from this book:

(1) _Upstream measures are often unpalatable_. There's often a sense that, of course, we want to prevent problems before they happen. But there are some things that get in the way:

- Cost. Often, the costs of upstream measures are concentrated and those that reap the benefits are dispersed. Incentive structures have to be carefully thought-out.
- Provability. It's often difficult to 'prove' that upstream interventions work because they are often longer-term
- Measurability. Reactionary interventions are really easy to monitor and measure. E.g.: the number of arrests by a police office vs. the number of crimes the police officer prevented; the latter is unknowable but obviously a better upstream intervention.
- Counter-factual. Perversely, we can get into a situation where we don't think some problems are problems because we handle them upstream so well. It can be hard to convince people that these are still problems that need to be taken seriously.

(2) The idea that _"every system is perfectly designed to get the results it gets"_

(3) _"the need for heroism is usually evidence of systems failure"_

(4) the need to look at balanced and complementary data to prevent gaming





----------

### chapter 1

- when you spend awhile responding to problems, you can overlook that they can often be prevented in the first place
- downstream = react to problems once they've already occurred. upstream = prevent problems from happening in the first place
- Reactionary, downstream responses tend to be more tangible and measurable
- Upstream interventions typically involve some sort of system thinking approach, i.e., trying to understand the system and look for points of leverage and how to change the overall system
- Upstream interventions can have a really big impact, but tend to be more difficult to measure
- Upstream efforts can sometimes backfire in ways we don't expect if we don't have a really good understanding of the system

## barriers to upstream thinking

### chapter 2 | problem blindness

- problem blindness - the belief that negative outcomes or problems are inevitable / out of our control
- "every system is perfectly designed to get the results it gets"
- blindness comes from lack of peripheral vision, looking for only the things we want / expect to see
- Also: habituation: we just get used to the ways things are, we stop seeing problems for problems. 

### chapter 3 | lack of ownership

- upstream work is chosen, not demanded, unlike downstream work. it's optional
- the attitude isn't "it's not my job to fix this problem;" the question should be "who's in the best position to fix this problem and will they step up"
- think about the problem like you're the only one responsible for it
- this chapter made me think about the lord's prayer and stoic writings and how it's actually really hard (at least for me) to draw then line between things I can change / have control over and that which I cannot

### chapter 4 | tunneling

- when time / money is scare, you adopt tunnel vision and only think about moving forwarding and keep afloat; there's no long-term thinking
- tunneling is a (bad) positive feedback lop
- "the need for heroism is usually evidence of systems failure"
- you need slack to escape the tunnel -- time or money to spend on problem solving [connection to some of the ideas for teams in _an elegant puzzle_]
- if possible, rebrand upstream work to be more urgent, a threat, something that must be immediately default with

## Questions to ask

### chapter 5 | How will you unite the right people?

- every stakeholder / person involved should have some role to play
- "surround the problem" -- attack it from various sides. not that downstream inventions are more narrowly focused / specialized
- you need to collection data to measure impact and learn what's working

### chapter 6 | How will you change the system?

- ultimately, the best upstrean intervention is changing the system
- there are all sorts of mechanisms in place that keep the system as it is and serve as a crutch / _ad hoc_ downstream intervention (e.g. food pantries). these organizations need to continue their work but also funnel some of their resources into advocating for a world where they are no longer needed

### chapter 7 | Where can you find leverage?

- some leverage points can from data. others are aimed at a fairly narrow, but critical, segment of the population
- aside: often we ask ROI questions about upstream efforts that we would never ask about downstream responses, esp. w/r/t problems in healthcare

### chapter 8 | How will you get early warning of the problem?

- in some cases, we may act too early and intervene in a way that it's helpful (e.g. thyroid cancer in s. korea)
- still, having mechanisms to intervene when early warning signs occur is important

### chapter 9 | How will you know you're succeeding?

- "ghost victories":
  - things got better, but for other reasons
  - the short-term measure and the longer-term goal are not aligned
  - the short-term goal became the mission
- think about whether you're substituting easy questions for the hard questions you actually want answers to
- data should be used for introspection / understanding, not enforcing a quota
- quantity measured should be paired with quality measures to prevent gaming metrics. it's pretty easy to fall into traps where members of the system can 'game' the metrics, so 'hedging' your metrics by looking at quality and quantity metrics makes sense

### chapter 10 | How will you avoid doing harm?

- need to consider the whole system, not just the part you care about
- what are the second or third order effects? consequences of the invention in 5 months? 5 years?
- interventions need to be designed with some kind of feedback so we know if we're amiss
- basically, second-order effect occur, so anticipate them, but know there will always be unanticipated impacts as well, and you need some kind of feedback loop to correct those effects you don't expect
- can you run a test / experiment first?

### chapter 11 | Who will pay for what doesn't happen?

- i.e., if upstream efforts work, there's no need for costly reactionary measures
- is some cases, the entity paying for the upstream intervention reaps all the benefits--this is the relatively easy case
- more commonly, the benefits of the intervention are spread around across a lot of stakeholders, and no single stakeholder wants to pay
- there are some interesting cases in healthcare where this problem is bring addressed
  - e.g. accountable care organizations: forecast the likely number of hospital visits within a given population, and pay doctors if the actual number of hospital visits is less
  - e.g. the insurer and the healthcare provider became the same company (KP). incentives align more often. doctors are paid based on a combination of quality and quantity metrics

### chapter 12 | distant and probable threats

- lots of example of preparation where
  - things were not bad (e.g. y2k)
  - things were bad, but could have been much worse (e.g. katrina)
- it's difficult to evaluate the counterfactuals in a lot of these scenarios
- "the prophet's dilemma" - a prophet makes a prediction, and because of the prediction, the thing she predicts never happens



### Final notes:

1. be "impatient for action and patient for outcomes"
2. macro starts with micro. small scale interventions and those that don't necessary scale (at least not at first) can still be useful and powerful, esp. by learning about the system
3. favor scoreboard over pills, i.e., prefer a system where you have the pulse on results as opposed to trying out an intervention and turning a blind eye until the experiment is over