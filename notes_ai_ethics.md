# AI Ethics

Coeckelbergh

This was an interesting read that raised all sorts of interesting questions. The primary things that stood out to me were (a) the idea that AI should or shouldn't be a mirror of the world, reflecting all its biases and thus exaserbating them, or whether we have an ethical obligation to inject 'biases' into datasets or change the algorithms we use to dismember these biases, and (b) that currating / collection a dataset, before you've built your model or done anything else, is a processes of abstracting from reality, and that process is never neutral.


Below I've just listed some notes and questions that I found particularlly interesting or compelling:

- At what point does an AI have moral agency? Does it have to be able to understand what it's doing for such agency to exist?
- Is AI capabile of being _more_ moral thans humans because it would be more dispassionate, not subject to emotions, e.g., than humans? IF we were to sede decision-making about important problems to AI, what would the consequences by for human agency and dignitity?
- Should someone be punished for kicking a robot dog? How aobut 'torturing' a humanoid robot?
- A lot of AI and data science relies on exploitation: exploitation of users whose data feeds into various models, exploitation of people who have to label datasets for various problems, exploitation in some cases to get and access data in the first place
- The idea of "the internet of toys" -- smart toys that are meant to interact with kids -- these toys are constantly using data from children to improve, learn, etc. Is this okay? Is the the child being exploited? Is a child in a position where it can 'give away' its data to improve a product? Are the parents responsible?
- Often times when dealing with AI, there's a choice between freedom and efficiency
- Is it okay  for AI to relfect the biases in society? Should we introduce biases to 'correct for' these biases in the world? Should AI be a mirror or a force to correct some of the problems in our world?
- Fears about robots taking our jobs sidestep many important questions: What value does work have in our society? What is the 'good life' we want to build towards? Can we use AI not to displace, but to build towards a more just and satisfying world?
- AI has the potential to exaserbate or allevaite many of the problems with the world today (e.g., income inequality, climate change, etc.), so needs to be discussed concurrently with these issues
- A dataset is an abstration from reality and _how_ we perform that abstraction has many consequences--even at the data collection stage, we are making ethical choices--"abstraction from reality is never neutral"